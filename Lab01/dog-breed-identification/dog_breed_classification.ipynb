{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import (\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Lambda,\n",
    ")\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/train/\"\n",
    "labels_path = \"./data/labels.csv\"\n",
    "img_size = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000bec180eb18c7604dcecc8fe0dba07</td>\n",
       "      <td>boston_bull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001513dfcb2ffafc82cccf4d8bbaba97</td>\n",
       "      <td>dingo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001cdf01b096e06d78e9e5112d419397</td>\n",
       "      <td>pekinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00214f311d5d2247d5dfe4fe24b2303d</td>\n",
       "      <td>bluetick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021f9ceb3235effd7fcde7f7538ed62</td>\n",
       "      <td>golden_retriever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id             breed\n",
       "0  000bec180eb18c7604dcecc8fe0dba07       boston_bull\n",
       "1  001513dfcb2ffafc82cccf4d8bbaba97             dingo\n",
       "2  001cdf01b096e06d78e9e5112d419397          pekinese\n",
       "3  00214f311d5d2247d5dfe4fe24b2303d          bluetick\n",
       "4  0021f9ceb3235effd7fcde7f7538ed62  golden_retriever"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(labels_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"image_path\"] = data_dir + df[\"id\"] + \".jpg\"\n",
    "df = df.drop(columns=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breeds = sorted(df[\"breed\"].unique().tolist())\n",
    "len(breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_num = dict(zip(breeds, range(len(breeds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Using Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_labels = df[\"breed\"]\n",
    "ml_data_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:21<00:00, 476.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    img_pixels = load_img(row[\"image_path\"], target_size=(32, 32, 3))\n",
    "    img_pixels = img_to_array(img_pixels)\n",
    "    img_pixels = img_pixels.ravel().reshape(1, -1)\n",
    "\n",
    "    row_data = pd.DataFrame(\n",
    "        img_pixels, columns=[f\"pixel{i}\" for i in range(len(img_pixels[0]))]\n",
    "    )\n",
    "    ml_data_list.append(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data = pd.concat(ml_data_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_train_data, ml_test_data, ml_train_label, ml_test_label = train_test_split(\n",
    "    ml_data.to_numpy(), ml_labels.to_numpy(), test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2045, 3072)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8177, 3072)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "knn.fit(ml_train_data, ml_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression()\n",
    "lg.fit(ml_train_data, ml_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lg = lg.predict(ml_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_performance(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    pre = precision_score(\n",
    "        y_true, y_pred, pos_label=\"positive\", average=\"micro\"\n",
    "    )\n",
    "    rec = recall_score(y_true, y_pred, pos_label=\"positive\", average=\"micro\")\n",
    "    return {\"Accuracy: \": acc, \"Precision: \": pre, \"Recall: \": rec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1521: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\metrics\\_classification.py:1521: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy: ': 0.025427872860635695,\n",
       " 'Precision: ': 0.025427872860635695,\n",
       " 'Recall: ': 0.025427872860635695}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation_performance(ml_test_label, y_pred_knn)\n",
    "evaluation_performance(ml_test_label, y_pred_lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Using CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all images into 1 array n value [n, x1, x2, x3]: <br>\n",
    "n: number of images<br>\n",
    "x1: height of image<br>\n",
    "x2: width of image<br>\n",
    "x3: depth of image(1 for grayscale image, 3 for color image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(data: pd.DataFrame, image_size: tuple):\n",
    "    X = np.zeros(\n",
    "        [len(data), image_size[0], image_size[1], image_size[2]],\n",
    "        dtype=np.uint8,\n",
    "    )\n",
    "    y = np.zeros([len(data), 1], dtype=np.uint8)\n",
    "    for idx, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        img_pixels = load_img(row[\"image_path\"], target_size=image_size)\n",
    "        X[idx] = img_pixels\n",
    "        y[idx] = class_to_num[row[\"breed\"]]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10222/10222 [00:09<00:00, 1087.61it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = image_processing(df, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 224, 224, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract feature using MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(image_size: tuple, data):\n",
    "    input_layer = Input(image_size)\n",
    "    processor = Lambda(preprocess_input)(input_layer)\n",
    "    base_model = MobileNetV2(\n",
    "        weights=\"imagenet\", include_top=False, input_shape=image_size\n",
    "    )(processor)\n",
    "    maxPool = MaxPooling2D()(base_model)\n",
    "    feature_extractor = Model(inputs=input_layer, outputs=maxPool)\n",
    "    feature_maps = feature_extractor.predict(data, batch_size=64, verbose=1)\n",
    "    return feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 77s 477ms/step\n"
     ]
    }
   ],
   "source": [
    "features = extract_feature(img_size, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10222, 3, 3, 1280)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelBinarizer()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    features, y, test_size=0.2, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1280)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 11520)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               2949376   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 120)               15480     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,997,752\n",
      "Trainable params: 2,997,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(X_train.shape[1:]))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(len(breeds), activation=\"softmax\"))\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 2s 10ms/step - loss: 3.6853 - accuracy: 0.2266 - val_loss: 1.9662 - val_accuracy: 0.5081\n",
      "Epoch 2/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 1.4674 - accuracy: 0.5990 - val_loss: 1.1660 - val_accuracy: 0.6699\n",
      "Epoch 3/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 1.0159 - accuracy: 0.7043 - val_loss: 1.0445 - val_accuracy: 0.6822\n",
      "Epoch 4/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.8151 - accuracy: 0.7515 - val_loss: 0.9507 - val_accuracy: 0.7125\n",
      "Epoch 5/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.6685 - accuracy: 0.7966 - val_loss: 0.9011 - val_accuracy: 0.7213\n",
      "Epoch 6/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.5513 - accuracy: 0.8327 - val_loss: 0.8796 - val_accuracy: 0.7350\n",
      "Epoch 7/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.4578 - accuracy: 0.8603 - val_loss: 0.8348 - val_accuracy: 0.7540\n",
      "Epoch 8/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3675 - accuracy: 0.8907 - val_loss: 0.8428 - val_accuracy: 0.7457\n",
      "Epoch 9/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.3030 - accuracy: 0.9164 - val_loss: 0.8373 - val_accuracy: 0.7418\n",
      "Epoch 10/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.2561 - accuracy: 0.9325 - val_loss: 0.8325 - val_accuracy: 0.7491\n",
      "Epoch 11/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.2040 - accuracy: 0.9506 - val_loss: 0.8195 - val_accuracy: 0.7535\n",
      "Epoch 12/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1679 - accuracy: 0.9622 - val_loss: 0.8543 - val_accuracy: 0.7526\n",
      "Epoch 13/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1367 - accuracy: 0.9724 - val_loss: 0.8534 - val_accuracy: 0.7570\n",
      "Epoch 14/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.1147 - accuracy: 0.9777 - val_loss: 0.8564 - val_accuracy: 0.7521\n",
      "Epoch 15/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.0917 - accuracy: 0.9850 - val_loss: 0.8646 - val_accuracy: 0.7614\n",
      "Epoch 16/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.0748 - accuracy: 0.9895 - val_loss: 0.8989 - val_accuracy: 0.7540\n",
      "Epoch 17/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.0664 - accuracy: 0.9906 - val_loss: 0.8907 - val_accuracy: 0.7575\n",
      "Epoch 18/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.0545 - accuracy: 0.9938 - val_loss: 0.9279 - val_accuracy: 0.7570\n",
      "Epoch 19/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.0494 - accuracy: 0.9951 - val_loss: 0.9166 - val_accuracy: 0.7628\n",
      "Epoch 20/20\n",
      "128/128 [==============================] - 1s 9ms/step - loss: 0.0427 - accuracy: 0.9957 - val_loss: 0.9165 - val_accuracy: 0.7643\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./data/test/\"\n",
    "img_test_files = os.listdir(test_path)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 478ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "japanese_spaniel\n",
      "1/1 [==============================] - 1s 675ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "samoyed\n",
      "WARNING:tensorflow:5 out of the last 165 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001AB1A62C3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 472ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "english_setter\n",
      "WARNING:tensorflow:6 out of the last 167 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001AB27123D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "newfoundland\n",
      "1/1 [==============================] - 0s 456ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "tibetan_terrier\n"
     ]
    }
   ],
   "source": [
    "for img in img_test_files:\n",
    "    file_name = os.path.join(test_path, img)\n",
    "    image_test = load_img(file_name, target_size=img_size)\n",
    "    image_test = np.array(image_test)\n",
    "    image_test = image_test.reshape((1, 224, 224, 3))\n",
    "    image_test = extract_feature(img_size, image_test)\n",
    "    breed_index = model.predict(image_test).argmax(axis=-1)\n",
    "    breed_label = [\n",
    "        label for label, index in class_to_num.items() if index == breed_index\n",
    "    ][0]\n",
    "\n",
    "    print(breed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
