{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pybboxes as pbx\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pre-processing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chosen dataset: https://www.kaggle.com/datasets/biancaferreira/african-wildlife <br>\n",
    "With this dataset I have 4 classes: buffalo, elephant, rhino, zebra. <br>\n",
    "Each class have image + annotation. First step I need to divide the dataset into processed_data folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "processed_data_path = \"./processed_data\"\n",
    "os.makedirs(processed_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_folder = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    columns=[\"class\", \"img_path\", \"x_min\", \"x_max\", \"y_min\", \"y_max\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_destination_folder = f\"{processed_data_path}/images/\"\n",
    "labels_destination_folder = f\"{processed_data_path}/labels/\"\n",
    "os.makedirs(images_destination_folder, exist_ok=True)\n",
    "os.makedirs(labels_destination_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "376it [00:01, 279.91it/s]00<?, ?it/s]\n",
      "376it [00:01, 221.05it/s]01<00:04,  1.35s/it]\n",
      "376it [00:01, 211.86it/s]03<00:03,  1.55s/it]\n",
      "376it [00:02, 155.58it/s]04<00:01,  1.66s/it]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:07<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "for label in tqdm(labels_folder):\n",
    "    org_path = f\"{data_path}/{label}\"\n",
    "    files = zip(os.listdir(org_path)[::2], os.listdir(org_path)[1::2])\n",
    "    for img, file in tqdm(files):\n",
    "        lb_file = f\"{org_path}/{file}\"\n",
    "        img_path = f\"{org_path}/{img}\"\n",
    "        numOfImage = len(os.listdir(images_destination_folder))\n",
    "        img_new_path = os.path.join(\n",
    "            images_destination_folder, f\"africa{numOfImage}.jpg\"\n",
    "        )\n",
    "        label_new_path = os.path.join(\n",
    "            labels_destination_folder, f\"africa{numOfImage}.txt\"\n",
    "        )\n",
    "        shutil.copy(\n",
    "            img_path,\n",
    "            img_new_path,\n",
    "        )\n",
    "        shutil.copy(\n",
    "            lb_file,\n",
    "            label_new_path,\n",
    "        )\n",
    "        lb_info = open(lb_file, \"r\").read().split()\n",
    "        objects = [[] for i in range(0, len(lb_info), 5)]\n",
    "        for idx, obj_info in enumerate(lb_info):\n",
    "            if idx % 5 == 0:\n",
    "                continue\n",
    "            else:\n",
    "                objects[idx // 5].append(obj_info)\n",
    "        for obj_list in objects:\n",
    "            if obj_list:\n",
    "                obj_details = obj_list\n",
    "\n",
    "                lb = label\n",
    "                x_min = float(obj_details[0])\n",
    "                x_max = float(obj_details[1])\n",
    "                y_min = float(obj_details[2])\n",
    "                y_max = float(obj_details[3])\n",
    "\n",
    "                row = {\n",
    "                    \"class\": lb,\n",
    "                    \"img_path\": img_new_path,\n",
    "                    \"x_min\": x_min,\n",
    "                    \"x_max\": x_max,\n",
    "                    \"y_min\": y_min,\n",
    "                    \"y_max\": y_max,\n",
    "                }\n",
    "\n",
    "                df = df.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>img_path</th>\n",
       "      <th>x_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_min</th>\n",
       "      <th>y_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buffalo</td>\n",
       "      <td>./processed_data/images/africa0.jpg</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.663017</td>\n",
       "      <td>0.617500</td>\n",
       "      <td>0.644769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buffalo</td>\n",
       "      <td>./processed_data/images/africa1.jpg</td>\n",
       "      <td>0.473515</td>\n",
       "      <td>0.508434</td>\n",
       "      <td>0.497592</td>\n",
       "      <td>0.838554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>buffalo</td>\n",
       "      <td>./processed_data/images/africa2.jpg</td>\n",
       "      <td>0.819167</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>buffalo</td>\n",
       "      <td>./processed_data/images/africa2.jpg</td>\n",
       "      <td>0.747500</td>\n",
       "      <td>0.472500</td>\n",
       "      <td>0.221667</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>buffalo</td>\n",
       "      <td>./processed_data/images/africa2.jpg</td>\n",
       "      <td>0.524167</td>\n",
       "      <td>0.543750</td>\n",
       "      <td>0.165000</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>zebra</td>\n",
       "      <td>./processed_data/images/africa1501.jpg</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>0.518919</td>\n",
       "      <td>0.387500</td>\n",
       "      <td>0.656757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>zebra</td>\n",
       "      <td>./processed_data/images/africa1502.jpg</td>\n",
       "      <td>0.548828</td>\n",
       "      <td>0.492568</td>\n",
       "      <td>0.330469</td>\n",
       "      <td>0.974324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>zebra</td>\n",
       "      <td>./processed_data/images/africa1503.jpg</td>\n",
       "      <td>0.205859</td>\n",
       "      <td>0.592568</td>\n",
       "      <td>0.221094</td>\n",
       "      <td>0.520270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>zebra</td>\n",
       "      <td>./processed_data/images/africa1503.jpg</td>\n",
       "      <td>0.431641</td>\n",
       "      <td>0.597297</td>\n",
       "      <td>0.242969</td>\n",
       "      <td>0.562162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>zebra</td>\n",
       "      <td>./processed_data/images/africa1503.jpg</td>\n",
       "      <td>0.579687</td>\n",
       "      <td>0.604730</td>\n",
       "      <td>0.123438</td>\n",
       "      <td>0.495946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2685 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                                img_path     x_min     x_max  \\\n",
       "0     buffalo     ./processed_data/images/africa0.jpg  0.560000  0.663017   \n",
       "1     buffalo     ./processed_data/images/africa1.jpg  0.473515  0.508434   \n",
       "2     buffalo     ./processed_data/images/africa2.jpg  0.819167  0.593750   \n",
       "3     buffalo     ./processed_data/images/africa2.jpg  0.747500  0.472500   \n",
       "4     buffalo     ./processed_data/images/africa2.jpg  0.524167  0.543750   \n",
       "...       ...                                     ...       ...       ...   \n",
       "2680    zebra  ./processed_data/images/africa1501.jpg  0.433594  0.518919   \n",
       "2681    zebra  ./processed_data/images/africa1502.jpg  0.548828  0.492568   \n",
       "2682    zebra  ./processed_data/images/africa1503.jpg  0.205859  0.592568   \n",
       "2683    zebra  ./processed_data/images/africa1503.jpg  0.431641  0.597297   \n",
       "2684    zebra  ./processed_data/images/africa1503.jpg  0.579687  0.604730   \n",
       "\n",
       "         y_min     y_max  \n",
       "0     0.617500  0.644769  \n",
       "1     0.497592  0.838554  \n",
       "2     0.148333  0.242500  \n",
       "3     0.221667  0.190000  \n",
       "4     0.165000  0.232500  \n",
       "...        ...       ...  \n",
       "2680  0.387500  0.656757  \n",
       "2681  0.330469  0.974324  \n",
       "2682  0.221094  0.520270  \n",
       "2683  0.242969  0.562162  \n",
       "2684  0.123438  0.495946  \n",
       "\n",
       "[2685 rows x 6 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"./processed_data/train/\"\n",
    "val_dir = \"./processed_data/val\"\n",
    "labels_path = \"./processed_data/labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(train_dir + \"/images\", exist_ok=True)\n",
    "os.makedirs(train_dir + \"/labels\", exist_ok=True)\n",
    "os.makedirs(val_dir + \"/images\", exist_ok=True)\n",
    "os.makedirs(val_dir + \"/labels\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(images_destination_folder)\n",
    "shuffle(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(files, ratio):\n",
    "    elements = len(files)\n",
    "    middle = int(elements * ratio)\n",
    "    return [files[:middle], files[middle:]]\n",
    "\n",
    "\n",
    "def copy_files(images_path, labels_path, destination_path, files):\n",
    "    for file_name in files:\n",
    "        file_name = file_name.split(\".\")[0]\n",
    "\n",
    "        src = images_path + f\"{file_name}.jpg\"\n",
    "        dst = destination_path + \"/images\"\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "        src = labels_path + f\"{file_name}.txt\"\n",
    "        dst = destination_path + \"/labels\"\n",
    "        shutil.copy(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.75\n",
    "train_files, val_files = split(files, train_ratio)\n",
    "\n",
    "copy_files(images_destination_folder, labels_path, train_dir, train_files)\n",
    "copy_files(images_destination_folder, labels_path, val_dir, val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_training = {idx: label for idx, label in enumerate(labels_folder)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'buffalo', 1: 'elephant', 2: 'rhino', 3: 'zebra'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./processed_data/africa_data.yaml\", \"w\") as f:\n",
    "    f.write(\"train: ./train/images\\n\")\n",
    "    f.write(\"val: ./val/images\\n\")\n",
    "    f.write(\"nc: 4\\n\")\n",
    "    f.write(f\"names: {class_training}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:00<00:00, 51.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.233 üöÄ Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./processed_data/africa_data.yaml, epochs=10, time=None, patience=50, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train2\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\train\\labels... 1128 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1128/1128 [00:01<00:00, 933.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\train\\images\\africa53.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\train\\images\\africa784.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\train\\images\\africa788.jpg: corrupt JPEG restored and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\val\\labels... 376 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 376/376 [00:00<00:00, 940.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\val\\images\\africa1438.jpg: corrupt JPEG restored and saved\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\train2\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.8279      2.151      1.048         16        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:57<00:00,  1.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.676      0.659      0.666      0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G     0.9292      1.421      1.092         12        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.688      0.577      0.641      0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G     0.9612      1.349      1.115         12        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:58<00:00,  1.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.787      0.501      0.614      0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G     0.9513      1.273      1.106         20        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.717      0.674      0.769      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G     0.9308      1.128      1.083         13        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.822      0.728      0.831      0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G     0.8548      1.055      1.059         16        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.817       0.74      0.844      0.629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G     0.8295     0.9555      1.043         10        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.874      0.774      0.875      0.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.8049     0.9006      1.035         13        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.885      0.792      0.895      0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.7386     0.7989      1.013         18        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.905      0.837      0.916      0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.7084     0.7464     0.9863         17        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71/71 [00:55<00:00,  1.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:09<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676       0.89      0.827      0.917      0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.183 hours.\n",
      "Optimizer stripped from runs\\detect\\train2\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train2\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train2\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.0.233 üöÄ Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:08<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676       0.89      0.827      0.917      0.732\n",
      "               buffalo        376        136       0.94      0.816      0.935        0.8\n",
      "              elephant        376        186      0.841      0.844      0.903      0.694\n",
      "                 rhino        376        144      0.903      0.837      0.934      0.774\n",
      "                 zebra        376        210      0.875       0.81      0.895      0.658\n",
      "Speed: 0.2ms preprocess, 11.2ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train2\u001b[0m\n",
      "Ultralytics YOLOv8.0.233 üöÄ Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "Model summary (fused): 168 layers, 3006428 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\val\\labels.cache... 376 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 376/376 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è C:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\processed_data\\val\\images\\africa1438.jpg: corrupt JPEG restored and saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:08<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        376        676      0.888      0.831      0.918      0.732\n",
      "               buffalo        376        136       0.94      0.824      0.937        0.8\n",
      "              elephant        376        186      0.853      0.844      0.905      0.698\n",
      "                 rhino        376        144      0.897      0.845      0.936      0.776\n",
      "                 zebra        376        210      0.865       0.81      0.892      0.656\n",
      "Speed: 0.2ms preprocess, 10.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train22\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(\n",
    "    data=\"./processed_data/africa_data.yaml\", epochs=10, imgsz=320\n",
    ")  # train the model\n",
    "results = model.val()  # evaluate model performance on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.233 üöÄ Python-3.9.13 torch-2.1.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\train2\\weights\\best.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 8, 2100) (5.9 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.15.0-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from onnx>=1.12.0) (1.24.3)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.12.0)\n",
      "  Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl.metadata (541 bytes)\n",
      "Downloading onnx-1.15.0-cp39-cp39-win_amd64.whl (14.3 MB)\n",
      "   ---------------------------------------- 14.3/14.3 MB 25.2 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.1-cp39-cp39-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 413.4/413.4 kB ? eta 0:00:00\n",
      "Installing collected packages: protobuf, onnx\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "Successfully installed onnx-1.15.0 protobuf-4.25.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 23.0s, installed 1 package: ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 23.6s, saved as 'runs\\detect\\train2\\weights\\best.onnx' (11.6 MB)\n",
      "\n",
      "Export complete (25.1s)\n",
      "Results saved to \u001b[1mC:\\Users\\ADMIN\\OneDrive - EPITA\\Computer-Vision-DSA-23\\Lab02\\runs\\detect\\train2\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\train2\\weights\\best.onnx imgsz=320  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\train2\\weights\\best.onnx imgsz=320 data=./processed_data/africa_data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs\\\\detect\\\\train2\\\\weights\\\\best.onnx'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format='onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
